{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0c11ecce5440a43dbf6e6c89564c0c092cfe18a29ee2325df151d53ab938c80d3",
   "display_name": "Python 3.8.10 64-bit ('tensorflow_m1': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "c11ecce5440a43dbf6e6c89564c0c092cfe18a29ee2325df151d53ab938c80d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finish importing 870 users\n"
     ]
    }
   ],
   "source": [
    "#import preprocessed per user dataset from onedrive prickled file\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = r\"/Users/nathantc5/OneDrive - The University of Sydney (Students)/CS48-CAPSTONE Project 2021 Sem1/dataset\"\n",
    "#create output dictionary\n",
    "data = {}\n",
    "\n",
    "N_user = 1000\n",
    "\n",
    "for u in os.listdir(path+'/user_7_days')[:N_user]: #import 100users\n",
    "\n",
    "    uid = int(u[:-7])\n",
    "    file_name = '/'.join([path,'/user_7_days', u])\n",
    "\n",
    "    try:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            dic = pickle.load(f)\n",
    "            data[uid] = dic[uid]\n",
    "    except:\n",
    "        print(dic)\n",
    "total_user = len(data.keys())\n",
    "print(\"Finish importing {} users\".format(total_user))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "weather = pd.read_csv('/'.join([path,'/weather_data.txt']), delimiter=\"\\t\")\n",
    "weather_np = weather.T.to_numpy()\n",
    "weather_dic = {20:weather_np[0], 21:weather_np[1], 22: weather_np[2], 23:weather_np[2], 24:weather_np[2],25:weather_np[3], 26:weather_np[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(47,)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "weather_dic[20][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create 2 function to extract the feature and label from array\n",
    "# def extract_time(lst):\n",
    "#     #extract first columns (session) from ROWS\n",
    "#     return [item[:1] for item in lst]\n",
    "\n",
    "# def extrac_place(lst):\n",
    "#     #extract second columns (mode base_station) from ROWS\n",
    "#     return [item[1] for item in lst]\n",
    "\n",
    "# def extract_app(lst):\n",
    "#     #extract app_usage columns from ROWS\n",
    "#     return np.array([item[2:] for item in lst])\n",
    "\n",
    "# # function to generate input for baseline model\n",
    "# def generate_baseline_input(data):\n",
    "\n",
    "#     data_neural = data\n",
    "\n",
    "#     all_prev_app = []\n",
    "#     all_target_app = []\n",
    "#     all_prev_t = []\n",
    "#     all_target_t = []\n",
    "\n",
    "#     day_id = [26] #using all the days for training\n",
    "\n",
    "#     # if candidate is None:\n",
    "#     candidate = data_neural.keys() #filter, and get user id\n",
    "\n",
    "#     #iterate all the users\n",
    "#     for u in candidate:\n",
    "#         #seperate feature and label list\n",
    "#         #get user's record\n",
    "#         sessions = data_neural[u]\n",
    "#         #sepearate to store pred and true\n",
    "#         user_prev_app = []\n",
    "#         user_target_app = []\n",
    "#         user_prev_t = []\n",
    "#         user_target_t = []\n",
    "\n",
    "#         for i in day_id:\n",
    "#             #call specific day\n",
    "#             session = sessions[i] #= data[u][i]\n",
    "#             # session is (48,2002)\n",
    "#             # idea here is to iterate over all 48 rows to extract the first columns and last 2000 columns\n",
    "\n",
    "#             #extract app_usage\n",
    "#             app_usage = np.array(extract_app(session))\n",
    "#             #extract time\n",
    "#             time_usage = np.array(extract_time(session))\n",
    "#             weather_day = weather_dic[i] #(48,)\n",
    "    \n",
    "\n",
    "\n",
    "#             #slice the app section to get app usage of ind 0 to second last rows (47,2000)\n",
    "#             prev_app = np.array(app_usage)[:-1,]\n",
    "#             #slice the app section to get app usage of ind 1 to last rows (47,2000)\n",
    "#             target_app = np.array(app_usage)[1:,]\n",
    "\n",
    "#             prev_t = np.array(time_usage)[:-1,]\n",
    "#             target_t = np.array(time_usage)[1:,]\n",
    "#             weather_day = weather_day[1:,] #(47,)\n",
    "\n",
    "#             user_prev_app.append(prev_app)\n",
    "#             user_target_app.append(target_app)\n",
    "#             user_prev_t.append(prev_t)\n",
    "#             user_target_t.append(target_t)\n",
    "#             # print(current_session[24])\n",
    "#             # print(previous_session[24])\n",
    "#         #after iterate the day, need to shift the feature columns upwards for prediction\n",
    "#         #also need to cut the last rows for labels\n",
    "#         #reshape: as orginal dataset were seperate by day,session,column:\n",
    "#         #feature:4(days),48sessions,2features\n",
    "#         #label:4(days),48sessions,2000 app usage(count)\n",
    "#         #reshape to flattern the data\n",
    "#         # user_prev = np.array(user_prev).reshape(-1,2000)\n",
    "#         # user_true = np.array(user_true).reshape(-1,2000)\n",
    "#         # print(user_prev[24])\n",
    "#         # print(user_true[24])\n",
    "#         #append per user's feature and label to all_user list\n",
    "#         all_prev_app.append(user_prev_app)\n",
    "#         all_target_app.append(user_target_app)\n",
    "#         all_prev_t.append(user_prev_t)\n",
    "#         all_target_t.append(user_target_t)\n",
    "\n",
    "#     all_prev_app= np.array(all_prev_app)\n",
    "#     all_target_app = np.array(all_target_app)\n",
    "#     all_prev_t = np.array(all_prev_t)\n",
    "#     all_target_t = np.array(all_target_t)\n",
    "\n",
    "#     return all_prev_app, all_target_app, all_prev_t, all_target_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 2 function to extract the feature and label from array\n",
    "#just to keep every clean as possible\n",
    "\n",
    "def extract_timeloc(lst):\n",
    "    #extract first 2 columns: session and mode location \n",
    "    return [item[:2] for item in lst]\n",
    "\n",
    "\n",
    "def extract_label(lst):\n",
    "    #extract the remaning columns\n",
    "    return [item[2:2002] for item in lst]\n",
    "\n",
    "\n",
    "def extract_poi(lst):\n",
    "    return [item[2002:] for item in lst]\n",
    "\n",
    "# function to generate input for baseline model\n",
    "# return(no.of users, rows of a users, feature dimension)\n",
    "def generate_baseline_input(data, mode):\n",
    "    data_neural = data\n",
    "    data_feature = []\n",
    "    data_label = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        day_id = [20, 21, 22] #the day for training\n",
    "\n",
    "    elif mode == 'test':\n",
    "        day_id = [26] # the day for testing\n",
    "\n",
    "    # if candidate is None:\n",
    "    candidate = data_neural.keys() #filter, and get user id\n",
    "\n",
    "    #iterate all the users\n",
    "    for u in candidate:\n",
    "        #seperate feature and label list\n",
    "        user_X_train = []\n",
    "        user_y_label = []\n",
    "        #get user's record\n",
    "        sessions = data_neural[u]\n",
    "        #seperate days for training and testing\n",
    "        for i in day_id:\n",
    "            #call specific day\n",
    "            session = data_neural[u][i]\n",
    "            #extract the part we want\n",
    "            # print(session.shape)\n",
    "\n",
    "            timeloc = np.array(extract_timeloc(session)[1:])\n",
    "            prev_app_count = np.array(extract_label(session)[:-1])\n",
    "            target = np.array(extract_label(session)[1:])\n",
    "            poi = np.array(extract_poi(session)[1:]) \n",
    "\n",
    "            weather_day = weather_dic[i]\n",
    "            weather_day = weather_day[1:]\n",
    "            weather_day = weather_day.reshape(-1,1)# weather_day from (47,) to (47,1)\n",
    "\n",
    "            # print(timeloc.shape)\n",
    "            # print(prev_app_count.shape)\n",
    "            # print(weather_day.shape)\n",
    "\n",
    "            #concate feature\n",
    "            user_feature = np.concatenate((timeloc,prev_app_count,weather_day,poi),axis = 1)\n",
    "            # print(user_feature.shape)\n",
    "            #append feature and label to corresponding list\n",
    "            user_X_train.append(user_feature)\n",
    "            user_y_label.append(target)\n",
    "            # print(user_feature.shape)\n",
    "\n",
    "        #reason for reshape here: group 1 user at 1 dimension when append to all_user list\n",
    "        user_X_train = np.array(user_X_train).reshape(-1,2020) #2 columns: timeloc, 2000 columns, app-usage, 17 columnns: POI, 1 columns:weather = 2020\n",
    "        user_y_label = np.array(user_y_label).reshape(-1,2000)\n",
    "        #append user_data to all_use list\n",
    "        data_feature.append(user_X_train)\n",
    "        data_label.append(user_y_label)\n",
    "    # no reshape here because when calling the variables it is easy to check how many user have been imported\\\n",
    "    # therefore it returns 3 dimenionsal data : (no.of users, rows of a users, feature dimension)\n",
    "    data_feature = np.array(data_feature)\n",
    "    data_label = np.array(data_label)\n",
    "\n",
    "    return data_feature, data_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(870, 141, 2020)\n(870, 141, 2000)\n(870, 47, 2020)\n(870, 47, 2000)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = generate_baseline_input(data, 'train')\n",
    "X_test, y_test = generate_baseline_input(data,'test')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "# print(\"X_train has {} user, each user has {} rows, each row has {} columns\". format(X_train.shape[0], X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "source": [
    "# Helper function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics function\n",
    "\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "def cal_ap( y_actual, y_pred, k ):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # Calculation of precision\n",
    "    num = 0\n",
    "    rank = 0\n",
    "    sumP = 0.0\n",
    "    for item in s_zip_topk:\n",
    "        rank += 1\n",
    "        if item[0] == 1:\n",
    "            num += 1\n",
    "            sumP += (num*1.0)/(rank*1.0)\n",
    "    ap = 0.0\n",
    "    if num > 0:\n",
    "        ap = sumP/(num*1.0)\n",
    "    return ap   # average precision\n",
    "# Take topk prediction and the ground truth\n",
    "\n",
    "def r_k(y_actual, y_pred, k, threshold):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # print(s_zip_topk)\n",
    "    actual, pred = zip(*s_zip_topk)\n",
    "    actual = np.where(np.array(actual) > threshold, 1, 0)\n",
    "    pred = np.array(pred)\n",
    "    pred_o = np.where(pred > threshold, 1, 0)\n",
    "    return actual, pred_o, pred\n",
    "\n",
    "\n",
    "def user_evaluation_metrics(y_pred, y_test):\n",
    "    total_auc = 0\n",
    "    total_map = 0\n",
    "    total_recall = 0\n",
    "    v_count = 0\n",
    "    count = 0\n",
    "\n",
    "    y_pred = np.where(y_pred >0,1,0)\n",
    "    y_test = np.where(y_test >0,1,0)\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_test.shape)\n",
    "\n",
    "    for i in range(y_test.shape[0]):\n",
    "\n",
    "        if (np.sum(y_test[i])> 0):\n",
    "            fpr, tpr, thresholds = skmetrics.roc_curve(y_test[i], y_pred[i], pos_label=1)\n",
    "            total_auc += skmetrics.auc(fpr, tpr)\n",
    "\n",
    "            actual, pred_o, pred = r_k(y_test[i], y_pred[i] ,5, 0.5)\n",
    "            total_recall += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5\n",
    "\n",
    "            total_map += cal_ap(y_test[i], y_pred[i], 2000)\n",
    "            #divide only the valid rows that has data\n",
    "            v_count +=1\n",
    "\n",
    "        else:\n",
    "            count +=1\n",
    "            pass\n",
    "            \n",
    "    if v_count != 0:\n",
    "        total_auc = total_auc / v_count\n",
    "        total_map = total_map / v_count\n",
    "        total_recall = total_recall / v_count\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return total_auc, total_map, total_recall, v_count, count\n",
    "\n",
    "\n",
    "def extract_prob(lst):\n",
    "    #first elements is prob for class 0, extract prob class 0 then 1- prob class 0 = prob class 1\n",
    "    return [(1-item[0]) for item in lst]\n",
    "\n",
    "# for i in range(2000):\n",
    "#     pred_BNB_prob[i] =  extract_prob(pred_BNB_prob[i])\n",
    "\n",
    "    # print('AUC: ', avg_auc / app_target.shape[0])\n",
    "    # print('MAP: ', avg_map / app_target.shape[0])\n",
    "    # print('Recall@5: ', avg_recall / app_target.shape[0])\n",
    "    # print('Skipped: {} rows, total: {} rows'.format(count, app_target.shape[0]))\n",
    "\n",
    "# fpr, tpr, thresholds = skmetrics.roc_curve(y_test, pred_MNB_prob, pos_label=1) # Collect the recall and false positive rate from all 2000 predictions\n",
    "# acc[0] += skmetrics.auc(fpr, tpr)\n",
    "# # acc[1] += cal_ap(truth, predict, 5)"
   ]
  },
  {
   "source": [
    "# NB Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=870.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e2215962c61439c827c428f5b7b3015"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Total user:  870\n",
      "AUC:  0.4440570202055382\n",
      "MAP:  0.17275248063212706\n",
      "Recall@5:  0.6675194836062958\n",
      "Rows: 11422 / 40890\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "total_user_auc = 0\n",
    "total_user_map = 0\n",
    "tota_user_recall = 0\n",
    "total_valid_rows = 0\n",
    "total_invalid_rows = 0\n",
    "\n",
    "for user in tqdm(range(total_user)):\n",
    "    user_X_train, user_y_train = X_train[user], y_train[user]\n",
    "    user_X_test, user_y_test = X_test[user], y_test[user]\n",
    "\n",
    "    nb_clf = MultiOutputClassifier(BernoulliNB()).fit(user_X_train, user_y_train)\n",
    "    prob = nb_clf.predict_proba(user_X_test)\n",
    "    \n",
    "    for apps in range(y_pred.shape[1]):\n",
    "        prob[apps] = extract_prob(prob[apps])\n",
    "    y_pred = np.array(prob).T\n",
    "    # print(y_pred.shape, user_y_test.shape)\n",
    "\n",
    "    a,m,r,v,c= user_evaluation_metrics(y_pred, user_y_test)\n",
    "    total_user_auc+=a\n",
    "    total_user_map+=m\n",
    "    tota_user_recall+=r\n",
    "    total_valid_rows+=v\n",
    "    total_invalid_rows+=c\n",
    "\n",
    "print('Total user: ', total_user)\n",
    "print('AUC: ', total_user_auc / total_user)\n",
    "print('MAP: ', total_user_map / total_user)\n",
    "print('Recall@5: ', tota_user_recall / total_user)\n",
    "print('Rows: {} / {}'.format(total_valid_rows, (total_valid_rows + total_invalid_rows)))\n",
    "\n"
   ]
  },
  {
   "source": [
    "# MLR Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=870.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9d3cf6be6294cd1b849e5ff908412f9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14\n",
      "38\n",
      "41\n",
      "67\n",
      "85\n",
      "90\n",
      "148\n",
      "189\n",
      "393\n",
      "421\n",
      "472\n",
      "592\n",
      "607\n",
      "679\n",
      "695\n",
      "771\n",
      "788\n",
      "\n",
      "Total user:  870\n",
      "AUC:  0.4354056469924877\n",
      "MAP:  0.3275366929244181\n",
      "Recall@5:  0.31708808507400504\n",
      "User: 870 / 870\n",
      "Rows: 11043 / 40890\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "valid_user = 0\n",
    "total_user_auc = 0\n",
    "total_user_map = 0\n",
    "tota_user_recall = 0\n",
    "total_valid_rows = 0\n",
    "total_invalid_rows = 0\n",
    "\n",
    "for user in tqdm(range(total_user)):\n",
    "    user_X_train, user_y_train = X_train[user], y_train[user]\n",
    "    user_X_test, user_y_test = X_test[user], y_test[user]\n",
    "\n",
    "    all0_column_ind = np.argwhere(np.all(user_y_train == 0, axis = 0))\n",
    "    # print(len(all0_column_ind))\n",
    "    user_y_train_rm = np.delete(user_y_train, all0_column_ind, axis=1)\n",
    "    user_y_test_rm = np.delete(user_y_test, all0_column_ind, axis=1)\n",
    "\n",
    "    try:\n",
    "\n",
    "        lr_clf = MultiOutputClassifier(LogisticRegression(multi_class = 'ovr', solver = 'saga', n_jobs = -1)).fit(user_X_train, user_y_train_rm)\n",
    "        prob = lr_clf.predict_proba(user_X_test)\n",
    "        for apps in range(user_y_train_rm.shape[1]):\n",
    "            prob[apps] = extract_prob(prob[apps])\n",
    "        y_pred = np.array(prob).T\n",
    "\n",
    "    except:\n",
    "        print(user)\n",
    "        # pass\n",
    "\n",
    "    # for apps in range(user_y_train_rm.shape[1]):\n",
    "    #     prob[apps] = extract_prob(prob[apps])\n",
    "\n",
    "    # y_pred = np.array(prob).T\n",
    "    # print(y_pred.shape, user_y_test.shape)\n",
    "\n",
    "    a,m,r,v,c= user_evaluation_metrics(y_pred, user_y_test_rm)\n",
    "\n",
    "    total_user_auc+=a\n",
    "    total_user_map+=m\n",
    "    tota_user_recall+=r\n",
    "    total_valid_rows+=v\n",
    "    total_invalid_rows+=c\n",
    "    valid_user+=1\n",
    "\n",
    "    # for apps in range(user_y_train_rm.shape[1]):\n",
    "    #     prob[apps] = extract_prob(prob[apps])\n",
    "    # y_pred = np.array(prob).T\n",
    "    # # print(y_pred.shape, user_y_test.shape)\n",
    "\n",
    "    # a,m,r,v,c= user_evaluation_metrics(y_pred, user_y_test_rm)\n",
    "\n",
    "    # total_user_auc+=a\n",
    "    # total_user_map+=m\n",
    "    # tota_user_recall+=r\n",
    "    # total_valid_rows+=v\n",
    "    # total_invalid_rows+=c\n",
    "\n",
    "print('Total user: ', valid_user)\n",
    "print('AUC: ', total_user_auc / valid_user)\n",
    "print('MAP: ', total_user_map / valid_user)\n",
    "print('Recall@5: ', tota_user_recall / valid_user)\n",
    "print('User: {} / {}'.format(valid_user, total_user))\n",
    "print('Rows: {} / {}'.format(total_valid_rows, (total_valid_rows + total_invalid_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}