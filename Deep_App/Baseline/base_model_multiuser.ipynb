{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0d835b9d6547198496337f4d2de04abf3395832a8b4aee7b55cf3102d3ef3dae9",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "#import preprocessed per user dataset from onedrive prickled file\n",
    "%load_ext autotime\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = r\"C:/Users/natha/OneDrive - The University of Sydney (Students)/CS48-CAPSTONE Project 2021 Sem1/dataset\"\n",
    "#create output dictionary\n",
    "#enter number of user here\n",
    "N_user = 1000\n",
    "\n",
    "data = {}\n",
    "for u in os.listdir(path+'/user_preprocessed_pickle')[:N_user]: #import N users\n",
    "\n",
    "    uid = int(u[:-7])\n",
    "    file_name = '/'.join([path,'/user_preprocessed_pickle', u])\n",
    "\n",
    "    try:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            dic = pickle.load(f)\n",
    "            data[uid] = dic[uid]\n",
    "    except:\n",
    "        print(dic)\n",
    "        \n",
    "total_user = len(data.keys())\n",
    "print(\"Imported {} users\".format(total_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#create 2 function to extract the feature and label from array\n",
    "def extract_feature(lst):\n",
    "    #extract first 2 columns: session and mode location \n",
    "    return [item[:2] for item in lst]\n",
    "\n",
    "def extract_label(lst):\n",
    "    #extract the remaning columns\n",
    "    return [item[2:] for item in lst]\n",
    "\n",
    "# function to generate input for baseline model\n",
    "def generate_baseline_input(data, mode):\n",
    "    data_neural = data\n",
    "    data_feature = []\n",
    "    data_label = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        day_id = [20, 21, 22, 25] #the day for training\n",
    "\n",
    "    elif mode == 'test':\n",
    "        day_id = [26] # the day for testing, should remove 26 which is left for real testing\n",
    "\n",
    "    # if candidate is None:\n",
    "    candidate = data_neural.keys() #filter, and get user id\n",
    "\n",
    "    #iterate all the users\n",
    "    for u in candidate:\n",
    "        #seperate feature and label list\n",
    "        user_X_train = []\n",
    "        user_y_label = []\n",
    "        #get user's record\n",
    "        sessions = data_neural[u]\n",
    "        #seperate days for training and testing\n",
    "        for i in day_id:\n",
    "            #call specific day\n",
    "            session = data_neural[u][i]\n",
    "            #append feature to feature list, label to label list\n",
    "            # user_feature.append(extract_feature(session))\n",
    "            # user_label.append(extract_label(session))\n",
    "            timeloc = extract_feature(session)[1:]\n",
    "            prev_app_count = extract_label(session)[:-1]\n",
    "            target = extract_label(session)[1:]\n",
    "            user_feature = np.concatenate((timeloc,prev_app_count),axis = 1)\n",
    "            user_X_train.append(user_feature)\n",
    "            user_y_label.append(target)\n",
    "        #after iterate the day, need to shift the feature columns upwards for prediction\n",
    "        #also need to cut the last rows for labels\n",
    "        #reshape: as orginal dataset were seperate by day,session,column:\n",
    "        #feature:4(days),48sessions,2features\n",
    "        #label:4(days),48sessions,2000 app usage(count)\n",
    "        #reshape to flattern the data\n",
    "        user_X_train = np.array(user_X_train).reshape(-1,2002)\n",
    "        user_y_label = np.array(user_y_label).reshape(-1,2000)\n",
    "        #append per user's feature and label to all_user list\n",
    "        data_feature.append(user_X_train)\n",
    "        data_label.append(user_y_label)\n",
    "\n",
    "    data_feature = np.array(data_feature)\n",
    "    data_label = np.array(data_label)\n",
    "\n",
    "    return data_feature, data_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(870, 188, 2002)\n(870, 188, 2000)\ntime: 5.33 s\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = generate_baseline_input(data,'train')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "#n users, n rows per user , 2 features/2000 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(163560, 2002)\n(163560, 2000)\ntime: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# X_train, y_train = generate_input(data, 'train')\n",
    "# reshape to merge user\n",
    "X_train = X_train.reshape(-1,2002)\n",
    "y_train = y_train.reshape(-1,2000)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(40890, 2002)\n(40890, 2000)\ntime: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = generate_baseline_input(data, 'test')\n",
    "X_test = X_test.reshape(-1,2002)\n",
    "y_test = y_test.reshape(-1,2000)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "# onehot encode the labels (turn count to one hot > used/not used)\n",
    "y_train = np.where(y_train > 0, 1, 0)\n",
    "y_test = np.where(y_test > 0, 1, 0)"
   ]
  },
  {
   "source": [
    "# Naive Bayes Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user the MultiOutputClassifier to fit one classifier to one target (so every rows of lable are trained with a seperate model)\n",
    "#pick Naive bayes purely because it is fast to train\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "clf_BNB = MultiOutputClassifier(BernoulliNB()).fit(X_train, y_train)\n",
    "# pred_BNB = clf_BNB.predict(X_test)\n",
    "# pred_BNB_prob = clf_BNB.predict_proba(X_test)\n",
    "# need the predict_proba to generate prob for each class(count)\n",
    "#as one hot coded it is just 2 class(0 for not use, 1 for use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_BNB = clf_BNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_BNB_prob = clf_BNB.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# clf_MNB = MultiOutputClassifier(MultinomialNB()).fit(X_train, y_train)\n",
    "# pred_MNB = clf_MNB.predict(X_test)\n",
    "# pred_MNB_prob = clf_MNB.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(40890, 2000)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#if train by count (Not one hot coded) might need this function to extract the probabiity of class 0, then 1- for the probability of using the app in next session\n",
    "\n",
    "def extract_prob(lst):\n",
    "    #first elements is prob for class 0, extract prob class 0 then 1- prob class 0 = prob class 1\n",
    "    \n",
    "    return [(1-item[0]) for item in lst]\n",
    "\n",
    "for i in range(2000):\n",
    "    pred_BNB_prob[i] =  extract_prob(pred_BNB_prob[i])\n",
    "\n",
    "#Transpose to match the shape\n",
    "pred_BNB_prob = np.array(pred_BNB_prob).T\n",
    "pred_BNB_prob.shape"
   ]
  },
  {
   "source": [
    "# MLR"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "378\ntime: 281 ms\n"
     ]
    }
   ],
   "source": [
    "all0_column_ind = np.argwhere(np.all(y_train == 0, axis = 0))\n",
    "print(len(all0_column_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rm = np.delete(y_train, all0_column_ind, axis=1)\n",
    "y_test_rm = np.delete(y_test, all0_column_ind, axis=1)"
   ]
  },
  {
   "source": [
    "# MLP (Ignore this part for now)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just random picked one model for testing, should have a lot of models\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier ##/\n",
    "# from sklearn.linear_model import LogisticRegression ##X\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_tanh = MultiOutputClassifier(MLPClassifier(random_state=1, max_iter=300, activation = 'tanh')).fit(X_train, y_train)\n",
    "pred_tanh_prob = clf_tanh.predict_proba(X_test)\n",
    "pred_tanh = clf_tanh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4700, 2000)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "#predict_proba > return the probability of every class \n",
    "#here class means the count occurs in train set\n",
    "#all we want is the prob of class 0, so we can 1- prob class 0 for class 1\n",
    "\n",
    "def extract_prob(lst):\n",
    "    return [(1-item[0]) for item in lst]\n",
    "\n",
    "for i in range(2000):\n",
    "    pred_tanh_prob[i] =  extract_prob(pred_tanh_prob[i])\n",
    "\n",
    "pred_tanh_prob = np.array(pred_tanh_prob).T\n",
    "pred_tanh_prob.shape"
   ]
  },
  {
   "source": [
    "# Evaluation\n",
    "## Follows the same evaluation metrics with DeepApp"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision MNB: 0.83533\n"
     ]
    }
   ],
   "source": [
    "# all_acc_session = []\n",
    "# y_test = y_test.astype(np.int64)\n",
    "# pred = pred_MNB.astype(np.int64)\n",
    "\n",
    "# for i in range(y_test.shape[0]):\n",
    "#     correct_count = 0\n",
    "#     pred_app_session = [app_ind for app_ind,app_count in enumerate(pred[i]) if app_count !=0]\n",
    "#     true_app_session = [app_ind for app_ind,app_count in enumerate(y_test[i]) if app_count !=0]\n",
    "#     if ((len(true_app_session) == 0) & (len(pred_app_session) == 0)):\n",
    "#         all_acc_session.append(1)\n",
    "#     else:\n",
    "#         for j in pred_app_session:\n",
    "#             if j in true_app_session:\n",
    "#                 correct_count +=1\n",
    "#         acc = correct_count/len(true_app_session)\n",
    "#         all_acc_session.append(acc)\n",
    "\n",
    "# print(\"Precision MNB: {:.5f}\".format(np.mean(all_acc_session)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ap( y_actual, y_pred, k ):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # Calculation of precision\n",
    "    num = 0\n",
    "    rank = 0\n",
    "    sumP = 0.0\n",
    "    for item in s_zip_topk:\n",
    "        rank += 1\n",
    "        if item[0] == 1:\n",
    "            num += 1\n",
    "            sumP += (num*1.0)/(rank*1.0)\n",
    "    ap = 0.0\n",
    "    if num > 0:\n",
    "        ap = sumP/(num*1.0)\n",
    "    return ap   # average precision\n",
    "# Take topk prediction and the ground truth\n",
    "\n",
    "\n",
    "def r_k(y_actual, y_pred, k, threshold):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # print(s_zip_topk)\n",
    "    actual, pred = zip(*s_zip_topk)\n",
    "    actual = np.where(np.array(actual) > threshold, 1, 0)\n",
    "    pred = np.array(pred)\n",
    "    pred_o = np.where(pred > threshold, 1, 0)\n",
    "    return actual, pred_o, pred\n",
    "\n",
    "# acc[3] += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5"
   ]
  },
  {
   "source": [
    "# Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC:  0.26008215917912597\nMAP:  0.13543381973315738\nRecall@5:  0.279334800684764\nSkipped: 29468 rows, total: 40890 rows\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "avg_auc = 0\n",
    "avg_map = 0\n",
    "avg_recall = 0\n",
    "count = 0\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    if (np.sum(y_test[i])> 0):\n",
    "        fpr, tpr, thresholds = skmetrics.roc_curve(y_test[i], pred_BNB_prob[i], pos_label=1)\n",
    "        avg_auc += skmetrics.auc(fpr, tpr)\n",
    "        actual, pred_o, pred = r_k(y_test[i], pred_BNB_prob[i] ,5, 5)\n",
    "        avg_recall += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5\n",
    "        avg_map += cal_ap(y_test[i], pred_BNB_prob[i], 5)\n",
    "    else:\n",
    "        count +=1\n",
    "        pass\n",
    "    \n",
    "print('AUC: ', avg_auc / y_test.shape[0])\n",
    "print('MAP: ', avg_map / y_test.shape[0])\n",
    "print('Recall@5: ', avg_recall / y_test.shape[0])\n",
    "print('Skipped: {} rows, total: {} rows'.format(count, y_test.shape[0]))\n",
    "\n",
    "# fpr, tpr, thresholds = skmetrics.roc_curve(y_test, pred_MNB_prob, pos_label=1) # Collect the recall and false positive rate from all 2000 predictions\n",
    "# acc[0] += skmetrics.auc(fpr, tpr)\n",
    "# # acc[1] += cal_ap(truth, predict, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics as skmetrics\n",
    "\n",
    "# avg_auc = 0\n",
    "# avg_map = 0\n",
    "# avg_recall = 0\n",
    "# count = 0\n",
    "\n",
    "# for i in range(y_test.shape[0]):\n",
    "#     if np.sum(pred_BNB_prob[i])> 0:\n",
    "#         fpr, tpr, thresholds = skmetrics.roc_curve(y_test[i], pred_BNB_prob[i], pos_label=1)\n",
    "#         avg_auc += skmetrics.auc(fpr, tpr)\n",
    "#         actual, pred_o, pred = r_k(y_test[i], pred_BNB_prob[i] ,5, 5)\n",
    "#         avg_recall += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5\n",
    "#         avg_map += cal_ap(y_test[i], pred_BNB_prob[i], 5)\n",
    "#     else:\n",
    "#         count +=1\n",
    "#         pass\n",
    "    \n",
    "# print('AUC: ', avg_auc / y_test.shape[0])\n",
    "# print('MAP: ', avg_map / y_test.shape[0])\n",
    "# print('Recall@5: ', avg_recall / y_test.shape[0])\n",
    "\n",
    "# print('{} rows has been skipped'.format(count))\n",
    "\n",
    "# # fpr, tpr, thresholds = skmetrics.roc_curve(y_test, pred_MNB_prob, pos_label=1) # Collect the recall and false positive rate from all 2000 predictions\n",
    "# # acc[0] += skmetrics.auc(fpr, tpr)\n",
    "# # # acc[1] += cal_ap(truth, predict, 5)"
   ]
  },
  {
   "source": [
    "# MLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC: 0.23330724614377835\nMAP 0.11360763888888885\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "avg_auc = 0\n",
    "avg_map = 0\n",
    "avg_recall = 0\n",
    "for i in range(2000):\n",
    "    if np.sum(y_test[i])> 0:\n",
    "        fpr, tpr, thresholds = skmetrics.roc_curve(y_test[i], pred_tanh_prob[i], pos_label=1)\n",
    "        avg_auc += skmetrics.auc(fpr, tpr)\n",
    "        actual, pred_o, pred = r_k(y_test[i], pred_tanh_prob[i] ,5, 5)\n",
    "        avg_recall += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5\n",
    "        avg_map += cal_ap(y_test[i], pred_tanh_prob[i], 5)\n",
    "    else:\n",
    "        pass\n",
    "print('AUC:', avg_auc / 4700)\n",
    "print('MAP', avg_map / 4700)\n",
    "print('Recall@5', avg_recall / 4700)\n",
    "\n",
    "# fpr, tpr, thresholds = skmetrics.roc_curve(y_test, pred_MNB_prob, pos_label=1) # Collect the recall and false positive rate from all 2000 predictions\n",
    "# acc[0] += skmetrics.auc(fpr, tpr)\n",
    "# # acc[1] += cal_ap(truth, predict, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}