{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0d835b9d6547198496337f4d2de04abf3395832a8b4aee7b55cf3102d3ef3dae9",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "bea709014dc7d5488003fd6d498d75fba76817fe887c69a6931445f4635fb88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# This is purely for M1 Macbook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from tensorflow.python.client import device_lib \n",
    "# print(device_lib.list_local_devices())\n",
    "# # from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# # from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "# # disable_eager_execution()\n",
    "# # mlcompute.set_mlc_device(device_name='gpu') \n",
    "# # print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import preprocessed per user dataset from onedrive prickled file\n",
    "# import pickle\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# path = r\"C:/Users/natha/OneDrive - The University of Sydney (Students)/CS48-CAPSTONE Project 2021 Sem1/dataset/\"\n",
    "# #create output dictionary\n",
    "# #enter number of user here\n",
    "# N_user = 10\n",
    "\n",
    "# data = {}\n",
    "# for u in os.listdir(path+'/user_preprocessed_pickle')[:N_user]: #import N users\n",
    "\n",
    "#     uid = int(u[:-7])\n",
    "#     file_name = '/'.join([path,'/user_preprocessed_pickle', u])\n",
    "\n",
    "#     try:\n",
    "#         with open(file_name, 'rb') as f:\n",
    "#             dic = pickle.load(f)\n",
    "#             data[uid] = dic[uid]\n",
    "#     except:\n",
    "#         print(dic)\n",
    "        \n",
    "# total_user = len(data.keys())\n",
    "# print(\"Imported {} users\".format(total_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create 2 function to extract the feature and label from array\n",
    "# #just to keep every clean as possible\n",
    "\n",
    "# def extract_feature(lst):\n",
    "#     #extract first 2 columns: session and mode location \n",
    "#     return [item[:2] for item in lst]\n",
    "\n",
    "# def extract_label(lst):\n",
    "#     #extract the remaning columns\n",
    "#     return [item[2:] for item in lst]\n",
    "\n",
    "\n",
    "\n",
    "# # function to generate input for baseline model\n",
    "# # return(no.of users, rows of a users, feature dimension)\n",
    "# def generate_baseline_input(data, mode):\n",
    "#     data_neural = data\n",
    "#     data_feature = []\n",
    "#     data_label = []\n",
    "\n",
    "#     if mode == 'train':\n",
    "#         day_id = [20, 21, 22, 25] #the day for training\n",
    "\n",
    "#     elif mode == 'test':\n",
    "#         day_id = [26] # the day for testing\n",
    "\n",
    "#     # if candidate is None:\n",
    "#     candidate = data_neural.keys() #filter, and get user id\n",
    "\n",
    "#     #iterate all the users\n",
    "#     for u in candidate:\n",
    "#         #seperate feature and label list\n",
    "#         user_X_train = []\n",
    "#         user_y_label = []\n",
    "#         #get user's record\n",
    "#         sessions = data_neural[u]\n",
    "#         #seperate days for training and testing\n",
    "#         for i in day_id:\n",
    "#             #call specific day\n",
    "#             session = data_neural[u][i]\n",
    "#             #extract the part we want\n",
    "#             timeloc = extract_feature(session)[1:]\n",
    "#             prev_app_count = extract_label(session)[:-1]\n",
    "#             target = extract_label(session)[1:]\n",
    "#             #concate feature\n",
    "#             user_feature = np.concatenate((timeloc,prev_app_count),axis = 1)\n",
    "#             #append feature and label to corresponding list\n",
    "#             user_X_train.append(user_feature)\n",
    "#             user_y_label.append(target)\n",
    "#         #reason for reshape here: group 1 user at 1 dimension when append to all_user list\n",
    "#         user_X_train = np.array(user_X_train).reshape(-1,2002)\n",
    "#         user_y_label = np.array(user_y_label).reshape(-1,2000)\n",
    "#         #append user_data to all_use list\n",
    "#         data_feature.append(user_X_train)\n",
    "#         data_label.append(user_y_label)\n",
    "#     # no reshape here because when calling the variables it is easy to check how many user have been imported\\\n",
    "#     # therefore it returns 3 dimenionsal data : (no.of users, rows of a users, feature dimension)\n",
    "#     data_feature = np.array(data_feature)\n",
    "#     data_label = np.array(data_label)\n",
    "\n",
    "#     return data_feature, data_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = generate_baseline_input(data, 'train')\n",
    "# X_test, y_test = generate_baseline_input(data,'test')\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print(\"X_train has {} user, each user has {} rows, each row has {} columns\". format(X_train.shape[0], X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(-1,2002)\n",
    "# y_train = y_train.reshape(-1,2000)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# X_test = X_test.reshape(-1,2002)\n",
    "# y_test = y_test.reshape(-1,2000)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.where(y_train > 0, 1, 0)\n",
    "# y_test = np.where(y_test > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 5000\n",
    "# SHUFFLE_BUFFER_SIZE = 2500\n",
    "\n",
    "# train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.001\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(2002, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(2000, activation=\"sigmoid\")\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#               metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "source": [
    "# Pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import timedelta\n",
    "\n",
    "torch.__version__\n",
    "\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU' if use_cuda else 'CPU')\n",
    "\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self, return_fmaps=False):\n",
    "      super(Simple_MLP, self).__init__()\n",
    "      self.fc1 = nn.Linear(2002, 1024)\n",
    "      self.act1 = nn.ReLU()\n",
    "\n",
    "      self.fc2 = nn.Linear(1024, 1024)\n",
    "      self.act2 = nn.ReLU()\n",
    "\n",
    "      self.fc3 = nn.Linear(1024,2000)\n",
    "      # self.act3 = nn.ReLU()\n",
    "\n",
    "      # self.fc4 = nn.Linear(512,2000)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        # x = self.act3(x)\n",
    "        \n",
    "        # x= self.fc4(x)\n",
    "\n",
    "        output = torch.sigmoid(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imported 870 users\n"
     ]
    }
   ],
   "source": [
    "#import preprocessed per user dataset from onedrive prickled file\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = r\"C:/Users/natha/OneDrive - The University of Sydney (Students)/CS48-CAPSTONE Project 2021 Sem1/dataset/\"\n",
    "#create output dictionary\n",
    "#enter number of user here\n",
    "N_user = 1000\n",
    "\n",
    "data = {}\n",
    "for u in os.listdir(path+'/user_preprocessed_pickle')[:N_user]: #import N users\n",
    "\n",
    "    uid = int(u[:-7])\n",
    "    file_name = '/'.join([path,'/user_preprocessed_pickle', u])\n",
    "\n",
    "    try:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            dic = pickle.load(f)\n",
    "            data[uid] = dic[uid]\n",
    "    except:\n",
    "        print(dic)\n",
    "        \n",
    "total_user = len(data.keys())\n",
    "print(\"Imported {} users\".format(total_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 2 function to extract the feature and label from array\n",
    "#just to keep every clean as possible\n",
    "\n",
    "def extract_feature(lst):\n",
    "    #extract first 2 columns: session and mode location \n",
    "    return [item[:2] for item in lst]\n",
    "\n",
    "def extract_label(lst):\n",
    "    #extract the remaning columns\n",
    "    return [item[2:] for item in lst]\n",
    "\n",
    "\n",
    "\n",
    "# function to generate input for baseline model\n",
    "# return(no.of users, rows of a users, feature dimension)\n",
    "def generate_baseline_input(data, mode):\n",
    "    data_neural = data\n",
    "    data_feature = []\n",
    "    data_label = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        day_id = [20, 21, 22] #the day for training\n",
    "\n",
    "    elif mode == 'test':\n",
    "        day_id = [26] # the day for testing\n",
    "\n",
    "    # if candidate is None:\n",
    "    candidate = data_neural.keys() #filter, and get user id\n",
    "\n",
    "    #iterate all the users\n",
    "    for u in candidate:\n",
    "        #seperate feature and label list\n",
    "        user_X_train = []\n",
    "        user_y_label = []\n",
    "        #get user's record\n",
    "        sessions = data_neural[u]\n",
    "        #seperate days for training and testing\n",
    "        for i in day_id:\n",
    "            #call specific day\n",
    "            session = data_neural[u][i]\n",
    "            #extract the part we want\n",
    "            timeloc = extract_feature(session)[1:]\n",
    "            prev_app_count = extract_label(session)[:-1]\n",
    "            target = extract_label(session)[1:]\n",
    "            #concate feature\n",
    "            user_feature = np.concatenate((timeloc,prev_app_count),axis = 1)\n",
    "            #append feature and label to corresponding list\n",
    "            user_X_train.append(user_feature)\n",
    "            user_y_label.append(target)\n",
    "        #reason for reshape here: group 1 user at 1 dimension when append to all_user list\n",
    "        user_X_train = np.array(user_X_train).reshape(-1,2002)\n",
    "        user_y_label = np.array(user_y_label).reshape(-1,2000)\n",
    "        #append user_data to all_use list\n",
    "        data_feature.append(user_X_train)\n",
    "        data_label.append(user_y_label)\n",
    "    # no reshape here because when calling the variables it is easy to check how many user have been imported\\\n",
    "    # therefore it returns 3 dimenionsal data : (no.of users, rows of a users, feature dimension)\n",
    "    data_feature = np.array(data_feature)\n",
    "    data_label = np.array(data_label)\n",
    "\n",
    "    return data_feature, data_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(870, 141, 2002)\n(870, 141, 2000)\n(870, 47, 2002)\n(870, 47, 2000)\nX_train has 870 user, each user has 141 rows, each row has 2002 columns\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = generate_baseline_input(data, 'train')\n",
    "X_test, y_test = generate_baseline_input(data,'test')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\"X_train has {} user, each user has {} rows, each row has {} columns\". format(X_train.shape[0], X_train.shape[1], X_train.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(-1,2002)\n",
    "# y_train = y_train.reshape(-1,2000)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# X_test = X_test.reshape(-1,2002)\n",
    "# y_test = y_test.reshape(-1,2000)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train > 0, 1, 0)\n",
    "y_test = np.where(y_test > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NpDataset(Dataset):\n",
    "  def __init__(self, array):\n",
    "    self.array = array\n",
    "  def __len__(self): return len(self.array)\n",
    "  def __getitem__(self, i): return self.array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = NpDataset(X_train)\n",
    "# y_train = NpDataset(y_train)\n",
    "# train_loader = DataLoader(X_train + y_train, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "train_dataset = TensorDataset(X_train,y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple_MLP().to(device)\n",
    "BCELoss = nn.BCELoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_k(y_actual, y_pred, k, threshold):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # print(s_zip_topk)\n",
    "    actual, pred = zip(*s_zip_topk)\n",
    "    actual = np.where(np.array(actual) > threshold, 1, 0)\n",
    "    pred = np.array(pred)\n",
    "    pred_o = np.where(pred > threshold, 1, 0)\n",
    "    return actual, pred_o, pred\n",
    "\n",
    "def cal_ap( y_actual, y_pred, k ):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # Calculation of precision\n",
    "    num = 0\n",
    "    rank = 0\n",
    "    sumP = 0.0\n",
    "    for item in s_zip_topk:\n",
    "        rank += 1\n",
    "        if item[0] == 1:\n",
    "            num += 1\n",
    "            sumP += (num*1.0)/(rank*1.0)\n",
    "    ap = 0.0\n",
    "    if num > 0:\n",
    "        ap = sumP/(num*1.0)\n",
    "    return ap   # average precision\n",
    "# Take topk prediction and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(log_interval, model, device, optimizer, loss_func, data, target):\n",
    "    '''\n",
    "    Train the model for a single iteration.\n",
    "    An iteration is when a single batch of data is passed forward and \n",
    "    backward through the neural network.\n",
    "    '''\n",
    "    data, target = data.to(device).float(), target.to(device).float()  # Move this batch of data to the specified device.\n",
    "    optimizer.zero_grad()  # Zero out the old gradients (so we only use new gradients for a new update iteration).\n",
    "    output = model(data)  # Forward the data through the model.\n",
    "    loss = loss_func(output, target)  # Calculate the loss\n",
    "    loss.backward()  # Backward the loss and calculate gradients for parameters.\n",
    "    optimizer.step()  # Update the parameters.\n",
    "    return loss\n",
    "\n",
    "def train_epoch(log_interval, model, device, train_loader, optimizer, epoch, loss_func):\n",
    "    '''\n",
    "    Train the model for an epoch.\n",
    "    An epoch is when the entire dataset is passed forward and \n",
    "    backward through the neural network for once.\n",
    "    The number of batches in a dataset is equal to number of iterations for one epoch.\n",
    "    '''\n",
    "    train_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # Iterate through the entire dataset to form an epoch.\n",
    "        loss = train_iter(log_interval, model, device, optimizer, loss_func, data, target)\n",
    "        train_loss.append(loss)  # Train for an iteration.\n",
    "        # if batch_idx % log_interval == 0:\n",
    "        \n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, loss_func):\n",
    "    '''\n",
    "    Testing the model on the entire test set.\n",
    "    '''\n",
    "    model.eval()  # Switch the model to evaluation mode, which prevents the dropout behavior.\n",
    "    test_loss = 0\n",
    "    outputs = []\n",
    "    with torch.no_grad():  # Because this is testing and no optimization is required, the gradients are not needed.\n",
    "        for data, target in test_loader:  # Iterate through the entire test set.\n",
    "            data, target = data.to(device).float(), target.to(device).float()  # Move this batch of data to the specified device.\n",
    "            output = model(data)  # Forward the data through the model.\n",
    "            test_loss += target.size(0)*loss_func(output, target).item() # Sum up batch loss\n",
    "            \n",
    "            # correct += pred.eq(target.view_as(pred)).item()  # Count the correct predictions.\n",
    "            output = output.cpu().data.numpy()\n",
    "            outputs.append(output)\n",
    "    test_loss /= len(test_loader.dataset)  # Average the loss on the entire testing set.\n",
    "    return np.array(outputs)\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "        test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [306/870 (75%)]\tLoss: 0.487489\n",
      "Train Epoch: 2 [306/870 (75%)]\tLoss: 0.465737\n",
      "Train Epoch: 3 [306/870 (75%)]\tLoss: 0.386968\n",
      "Train Epoch: 4 [306/870 (75%)]\tLoss: 0.272016\n",
      "Train Epoch: 5 [306/870 (75%)]\tLoss: 0.156018\n",
      "Train Epoch: 6 [306/870 (75%)]\tLoss: 0.097033\n",
      "Train Epoch: 7 [306/870 (75%)]\tLoss: 0.081190\n",
      "Train Epoch: 8 [306/870 (75%)]\tLoss: 0.080403\n",
      "Train Epoch: 9 [306/870 (75%)]\tLoss: 0.082358\n",
      "Train Epoch: 10 [306/870 (75%)]\tLoss: 0.082543\n",
      "Train Epoch: 11 [306/870 (75%)]\tLoss: 0.092466\n",
      "Train Epoch: 12 [306/870 (75%)]\tLoss: 0.082449\n",
      "Train Epoch: 13 [306/870 (75%)]\tLoss: 0.082118\n",
      "Train Epoch: 14 [306/870 (75%)]\tLoss: 0.080580\n",
      "Train Epoch: 15 [306/870 (75%)]\tLoss: 0.082915\n",
      "Train Epoch: 16 [306/870 (75%)]\tLoss: 0.083691\n",
      "Train Epoch: 17 [306/870 (75%)]\tLoss: 0.083916\n",
      "Train Epoch: 18 [306/870 (75%)]\tLoss: 0.084004\n",
      "Train Epoch: 19 [306/870 (75%)]\tLoss: 0.084049\n",
      "Train Epoch: 20 [306/870 (75%)]\tLoss: 0.084076\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  train_epoch(0, model, device, train_loader, optimizer, i, BCELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(870, 47, 2000)\n(870, 47, 2000)\n"
     ]
    }
   ],
   "source": [
    "y_pred = test(model, device, test_loader, BCELoss)\n",
    "y_pred = y_pred.reshape(total_user,-1,2000)\n",
    "# y_pred = y_test.reshape()\n",
    "y_test = y_test.cpu().numpy()\n",
    "print(y_pred.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=870.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30e8558d781f4b29a7bb8a101821faf3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTotal user:  870\nAUC:  0.4518664956673652\nMAP:  0.16954816863261718\nRecall@5:  0.6671241748130954\nRows: 11422 / 40890\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "total_avg_auc = 0\n",
    "total_avg_map = 0\n",
    "total_avg_recall = 0\n",
    "total_vcount = 0\n",
    "total_count = 0\n",
    "\n",
    "for u in tqdm(range(total_user)):\n",
    "    count = 0\n",
    "    v_count = 0\n",
    "    avg_auc = 0\n",
    "    avg_map = 0\n",
    "    avg_recall = 0\n",
    "\n",
    "    for i in range(len(y_test[u])):\n",
    "        if (np.sum(y_test[u][i])> 0):\n",
    "            # print(y_test[u][i].shape)\n",
    "            fpr, tpr, thresholds = skmetrics.roc_curve(y_test[u][i], y_pred[u][i], pos_label=1)\n",
    "            avg_auc += skmetrics.auc(fpr, tpr)\n",
    "            actual, pred_o, pred = r_k(y_test[u][i], y_pred[u][i] ,5, 0.5)\n",
    "            avg_recall += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5\n",
    "            avg_map += cal_ap(y_test[u][i], y_pred[u][i], len(y_pred[u][i]))\n",
    "            v_count +=1\n",
    "        else:\n",
    "            count +=1\n",
    "            pass\n",
    "\n",
    "    total_vcount +=v_count\n",
    "    total_count +=count\n",
    "    if v_count !=0:\n",
    "        total_avg_auc += avg_auc/v_count\n",
    "        total_avg_map += avg_map/v_count\n",
    "        total_avg_recall += avg_recall/v_count\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print('Total user: ', total_user)\n",
    "print('AUC: ', total_avg_auc / total_user)\n",
    "print('MAP: ', total_avg_map / total_user)\n",
    "print('Recall@5: ', total_avg_recall / total_user)\n",
    "print('Rows: {} / {}'.format(total_vcount, (total_vcount + total_count)))# print('Skipped: {} rows, total: {} rows'.format(count, y_test.shape[0]))\n",
    "\n",
    "# fpr, tpr, thresholds = skmetrics.roc_curve(y_test, pred_MNB_prob, pos_label=1) # Collect the recall and false positive rate from all 2000 predictions\n",
    "# acc[0] += skmetrics.auc(fpr, tpr)\n",
    "# # acc[1] += cal_ap(truth, predict, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}