{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0d835b9d6547198496337f4d2de04abf3395832a8b4aee7b55cf3102d3ef3dae9",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "c11ecce5440a43dbf6e6c89564c0c092cfe18a29ee2325df151d53ab938c80d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MRU Base model\n",
    "## Evaluate per user"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imported 870 users\n"
     ]
    }
   ],
   "source": [
    "#import preprocessed per user dataset from onedrive prickled file\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = r\"C:/Users/natha/OneDrive - The University of Sydney (Students)/CS48-CAPSTONE Project 2021 Sem1/dataset\"\n",
    "#create output dictionary\n",
    "data = {}\n",
    "\n",
    "N_user = 1000\n",
    "\n",
    "for u in os.listdir(path+'/user_preprocessed_pickle')[:N_user]: #import 100users\n",
    "\n",
    "    uid = int(u[:-7])\n",
    "    file_name = '/'.join([path,'/user_preprocessed_pickle', u])\n",
    "\n",
    "    try:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            dic = pickle.load(f)\n",
    "            data[uid] = dic[uid]\n",
    "    except:\n",
    "        print(dic)\n",
    "total_user = len(data.keys())\n",
    "print(\"Imported {} users\".format(total_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 2 function to extract the feature and label from array\n",
    "def extract_time(lst):\n",
    "    #extract first columns (session) from ROWS\n",
    "    return [item[:1] for item in lst]\n",
    "\n",
    "def extrac_place(lst):\n",
    "    #extract second columns (mode base_station) from ROWS\n",
    "    return [item[1] for item in lst]\n",
    "\n",
    "def extract_app(lst):\n",
    "    #extract app_usage columns from ROWS\n",
    "    return np.array([item[2:] for item in lst])\n",
    "\n",
    "# function to generate input for baseline model\n",
    "def generate_baseline_input(data):\n",
    "\n",
    "    data_neural = data\n",
    "\n",
    "    all_prev_app = []\n",
    "    all_target_app = []\n",
    "    all_prev_t = []\n",
    "    all_target_t = []\n",
    "\n",
    "    day_id = [26] #using all the days for training\n",
    "\n",
    "    # if candidate is None:\n",
    "    candidate = data_neural.keys() #filter, and get user id\n",
    "\n",
    "    #iterate all the users\n",
    "    for u in candidate:\n",
    "        #seperate feature and label list\n",
    "        #get user's record\n",
    "        sessions = data_neural[u]\n",
    "        #sepearate to store pred and true\n",
    "        user_prev_app = []\n",
    "        user_target_app = []\n",
    "        user_prev_t = []\n",
    "        user_target_t = []\n",
    "\n",
    "        for i in day_id:\n",
    "            #call specific day\n",
    "            session = sessions[i] #= data[u][i]\n",
    "            # session is (48,2002)\n",
    "            # idea here is to iterate over all 48 rows to extract the first columns and last 2000 columns\n",
    "\n",
    "            #extract app_usage\n",
    "            app_usage = np.array(extract_app(session))\n",
    "            #extract time\n",
    "            time_usage = np.array(extract_time(session))\n",
    "\n",
    "            #slice the app section to get app usage of ind 0 to second last rows (47,2000)\n",
    "            prev_app = np.array(app_usage)[:-1,]\n",
    "            #slice the app section to get app usage of ind 1 to last rows (47,2000)\n",
    "            target_app = np.array(app_usage)[1:,]\n",
    "\n",
    "            prev_t = np.array(time_usage)[:-1,]\n",
    "            target_t = np.array(time_usage)[1:,]\n",
    "\n",
    "            user_prev_app.append(prev_app)\n",
    "            user_target_app.append(target_app)\n",
    "            user_prev_t.append(prev_t)\n",
    "            user_target_t.append(target_t)\n",
    "            # print(current_session[24])\n",
    "            # print(previous_session[24])\n",
    "        #after iterate the day, need to shift the feature columns upwards for prediction\n",
    "        #also need to cut the last rows for labels\n",
    "        #reshape: as orginal dataset were seperate by day,session,column:\n",
    "        #feature:4(days),48sessions,2features\n",
    "        #label:4(days),48sessions,2000 app usage(count)\n",
    "        #reshape to flattern the data\n",
    "        # user_prev = np.array(user_prev).reshape(-1,2000)\n",
    "        # user_true = np.array(user_true).reshape(-1,2000)\n",
    "        # print(user_prev[24])\n",
    "        # print(user_true[24])\n",
    "        #append per user's feature and label to all_user list\n",
    "        all_prev_app.append(user_prev_app)\n",
    "        all_target_app.append(user_target_app)\n",
    "        all_prev_t.append(user_prev_t)\n",
    "        all_target_t.append(user_target_t)\n",
    "\n",
    "    all_prev_app= np.array(all_prev_app)\n",
    "    all_target_app = np.array(all_target_app)\n",
    "    all_prev_t = np.array(all_prev_t)\n",
    "    all_target_t = np.array(all_target_t)\n",
    "\n",
    "    return all_prev_app, all_target_app, all_prev_t, all_target_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(870, 1, 47, 2000) (870, 1, 47, 2000) (870, 1, 47, 1) (870, 1, 47, 1)\n"
     ]
    }
   ],
   "source": [
    "app_np, app_target, tim_np, ptim_np = generate_baseline_input(data)\n",
    "print(app_np.shape, app_target.shape, tim_np.shape, ptim_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(870, 47, 2000) (870, 47, 2000) (870, 47, 1) (870, 47, 1)\n"
     ]
    }
   ],
   "source": [
    "app_np = app_np.reshape(total_user,-1,2000)\n",
    "app_target = app_target.reshape(total_user,-1,2000)\n",
    "tim_np = tim_np.reshape(total_user,-1,1)\n",
    "ptim_np = ptim_np.reshape(total_user,-1,1)\n",
    "print(app_np.shape, app_target.shape, tim_np.shape, ptim_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics function\n",
    "\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "def cal_ap( y_actual, y_pred, k ):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # Calculation of precision\n",
    "    num = 0\n",
    "    rank = 0\n",
    "    sumP = 0.0\n",
    "    for item in s_zip_topk:\n",
    "        rank += 1\n",
    "        if item[0] == 1:\n",
    "            num += 1\n",
    "            sumP += (num*1.0)/(rank*1.0)\n",
    "    ap = 0.0\n",
    "    if num > 0:\n",
    "        ap = sumP/(num*1.0)\n",
    "    return ap   # average precision\n",
    "# Take topk prediction and the ground truth\n",
    "\n",
    "def r_k(y_actual, y_pred, k, threshold):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # print(s_zip_topk)\n",
    "    actual, pred = zip(*s_zip_topk)\n",
    "    actual = np.where(np.array(actual) > threshold, 1, 0)\n",
    "    pred = np.array(pred)\n",
    "    pred_o = np.where(pred > threshold, 1, 0)\n",
    "    return actual, pred_o, pred\n",
    "\n",
    "\n",
    "def user_evaluation_metrics(y_pred, y_test):\n",
    "    total_auc = 0\n",
    "    total_map = 0\n",
    "    total_recall = 0\n",
    "    v_count = 0\n",
    "    count = 0\n",
    "\n",
    "    y_pred = np.where(y_pred >0,1,0)\n",
    "    y_test = np.where(y_test >0,1,0)\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_test.shape)\n",
    "\n",
    "    for i in range(y_test.shape[0]):\n",
    "\n",
    "        if (np.sum(y_test[i])> 0):\n",
    "            fpr, tpr, thresholds = skmetrics.roc_curve(y_test[i], y_pred[i], pos_label=1)\n",
    "            total_auc += skmetrics.auc(fpr, tpr)\n",
    "            actual, pred_o, pred = r_k(y_test[i], y_pred[i] ,5, 0.5)\n",
    "            total_recall += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5\n",
    "            total_map += cal_ap(y_test[i], y_pred[i], 2000)\n",
    "            #divide only the valid rows that has data\n",
    "            v_count +=1\n",
    "\n",
    "        else:\n",
    "            count +=1\n",
    "            pass\n",
    "            \n",
    "    if v_count != 0:\n",
    "        total_auc = total_auc / v_count\n",
    "        total_map = total_map / v_count\n",
    "        total_recall = total_recall / v_count\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return total_auc, total_map, total_recall, v_count, count\n",
    "        \n",
    "    # print('AUC: ', avg_auc / app_target.shape[0])\n",
    "    # print('MAP: ', avg_map / app_target.shape[0])\n",
    "    # print('Recall@5: ', avg_recall / app_target.shape[0])\n",
    "    # print('Skipped: {} rows, total: {} rows'.format(count, app_target.shape[0]))\n",
    "\n",
    "# fpr, tpr, thresholds = skmetrics.roc_curve(y_test, pred_MNB_prob, pos_label=1) # Collect the recall and false positive rate from all 2000 predictions\n",
    "# acc[0] += skmetrics.auc(fpr, tpr)\n",
    "# # acc[1] += cal_ap(truth, predict, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=870.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0277c28a8d84b2a93d1f1b2f9bd2d6c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\natha\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "Total user:  870\n",
      "AUC:  0.563273208459653\n",
      "MAP:  0.2847653478424465\n",
      "Recall@5:  0.5858401073724266\n",
      "Rows: 11422 / 40890\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "total_user_auc = 0\n",
    "total_user_map = 0\n",
    "total_user_recall = 0\n",
    "total_valid_rows = 0\n",
    "total_non_valid_rows = 0\n",
    "\n",
    "for i in tqdm(range(total_user)):\n",
    "    app_score = np.zeros([len(app_target[i]), 2000])\n",
    "    for t in range(len(tim_np[i])):\n",
    "        if(ptim_np[i][t] == (tim_np[i][t]+1)):\n",
    "            app_score[t] = app_np[i][t]\n",
    "    # print(app_score.shape, app_target.shape)\n",
    "    # print(app_score.shape, app_target[i].shape)\n",
    "    # print(len(app_target[i]))\n",
    "    user_auc, user_map, user_recall, valid_rows, non_valid_rows = user_evaluation_metrics(app_score, app_target[i])\n",
    "\n",
    "    total_user_auc+= user_auc\n",
    "    total_user_map+= user_map\n",
    "    total_user_recall+= user_recall\n",
    "\n",
    "    total_valid_rows += valid_rows\n",
    "    total_non_valid_rows += non_valid_rows\n",
    "\n",
    "print('Total user: ', total_user)\n",
    "print('AUC: ', total_user_auc / total_user)\n",
    "print('MAP: ', total_user_map / total_user)\n",
    "print('Recall@5: ', total_user_recall / total_user)\n",
    "print('Rows: {} / {}'.format(total_valid_rows, (total_valid_rows + total_non_valid_rows)))\n",
    "\n",
    "\n",
    "            # print(app_np[i].shape)\n",
    "            # print(app_target[i].shape)\n",
    "            # print(tim_np[i].shape)\n",
    "            # print(ptim_np[i].shape)"
   ]
  },
  {
   "source": [
    "# MRU\n",
    "## Evaluate whole dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imported 870 users\n"
     ]
    }
   ],
   "source": [
    "#import preprocessed per user dataset from onedrive prickled file\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = r\"C:/Users/natha/OneDrive - The University of Sydney (Students)/CS48-CAPSTONE Project 2021 Sem1/dataset\"\n",
    "#create output dictionary\n",
    "data = {}\n",
    "\n",
    "N_user = 1000\n",
    "\n",
    "for u in os.listdir(path+'/user_preprocessed_pickle')[:N_user]: #import 100users\n",
    "\n",
    "    uid = int(u[:-7])\n",
    "    file_name = '/'.join([path,'/user_preprocessed_pickle', u])\n",
    "\n",
    "    try:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            dic = pickle.load(f)\n",
    "            data[uid] = dic[uid]\n",
    "    except:\n",
    "        print(dic)\n",
    "\n",
    "print(\"Imported {} users\".format(len(data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 2 function to extract the feature and label from array\n",
    "def extract_time(lst):\n",
    "    #extract first 2 columns: session and mode location \n",
    "    return [item[:1] for item in lst]\n",
    "\n",
    "def extract_app(lst):\n",
    "    #extract the remaning columns\n",
    "    return np.array([item[2:] for item in lst])\n",
    "\n",
    "# function to generate input for baseline model\n",
    "def generate_baseline_input(data):\n",
    "\n",
    "    data_neural = data\n",
    "\n",
    "    all_prev_app = []\n",
    "    all_target_app = []\n",
    "    all_prev_t = []\n",
    "    all_target_t = []\n",
    "\n",
    "    day_id = [26] #using all the days for training\n",
    "\n",
    "    # if candidate is None:\n",
    "    candidate = data_neural.keys() #filter, and get user id\n",
    "\n",
    "    #iterate all the users\n",
    "    for u in candidate:\n",
    "        #seperate feature and label list\n",
    "        #get user's record\n",
    "        sessions = data_neural[u]\n",
    "        #sepearate to store pred and true\n",
    "        user_prev_app = []\n",
    "        user_target_app = []\n",
    "        user_prev_t = []\n",
    "        user_target_t = []\n",
    "\n",
    "        for i in day_id:\n",
    "            #call specific day\n",
    "            session = sessions[i]\n",
    "            #append feature to feature list, label to label list\n",
    "            app_usage = np.array(extract_app(session))\n",
    "            time_usage = np.array(extract_time(session))\n",
    "            # print(app_usage)\n",
    "            #basically just just the previous session as the \"pred\" of next session\n",
    "            prev_app = np.array(app_usage)[:-1,]\n",
    "            target_app = np.array(app_usage)[1:,]\n",
    "\n",
    "            prev_t = np.array(time_usage)[:-1,]\n",
    "            target_t = np.array(time_usage)[1:,]\n",
    "\n",
    "            user_prev_app.append(prev_app)\n",
    "            user_target_app.append(target_app)\n",
    "            user_prev_t.append(prev_t)\n",
    "            user_target_t.append(target_t)\n",
    "            # print(current_session[24])\n",
    "            # print(previous_session[24])\n",
    "        #after iterate the day, need to shift the feature columns upwards for prediction\n",
    "        #also need to cut the last rows for labels\n",
    "        #reshape: as orginal dataset were seperate by day,session,column:\n",
    "        #feature:4(days),48sessions,2features\n",
    "        #label:4(days),48sessions,2000 app usage(count)\n",
    "        #reshape to flattern the data\n",
    "        # user_prev = np.array(user_prev).reshape(-1,2000)\n",
    "        # user_true = np.array(user_true).reshape(-1,2000)\n",
    "        # print(user_prev[24])\n",
    "        # print(user_true[24])\n",
    "        #append per user's feature and label to all_user list\n",
    "        all_prev_app.append(user_prev_app)\n",
    "        all_target_app.append(user_target_app)\n",
    "        all_prev_t.append(user_prev_t)\n",
    "        all_target_t.append(user_target_t)\n",
    "\n",
    "    all_prev_app= np.array(all_prev_app)\n",
    "    all_target_app = np.array(all_target_app)\n",
    "    all_prev_t = np.array(all_prev_t)\n",
    "    all_target_t = np.array(all_target_t)\n",
    "\n",
    "    return all_prev_app, all_target_app, all_prev_t, all_target_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(870, 1, 47, 2000) (870, 1, 47, 2000) (870, 1, 47, 1) (870, 1, 47, 1)\n"
     ]
    }
   ],
   "source": [
    "app_np, app_target, tim_np, ptim_np = generate_baseline_input(data)\n",
    "print(app_np.shape, app_target.shape, tim_np.shape, ptim_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(40890, 2000)\n(40890, 2000)\n"
     ]
    }
   ],
   "source": [
    "app_np = app_np.reshape(-1,2000)\n",
    "app_target = app_target.reshape(-1,2000)\n",
    "# app_np = np.where(app_np > 0, 1,0)\n",
    "# app_target = np.where(app_target > 0, 1,0)\n",
    "print(app_np.shape)\n",
    "print(app_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "from tqdm.notebook import tqdm\n",
    "def cal_ap( y_actual, y_pred, k ):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # Calculation of precision\n",
    "    num = 0\n",
    "    rank = 0\n",
    "    sumP = 0.0\n",
    "    for item in s_zip_topk:\n",
    "        rank += 1\n",
    "        if item[0] == 1:\n",
    "            num += 1\n",
    "            sumP += (num*1.0)/(rank*1.0)\n",
    "    ap = 0.0\n",
    "    if num > 0:\n",
    "        ap = sumP/(num*1.0)\n",
    "    return ap   # average precision\n",
    "# Take topk prediction and the ground truth\n",
    "\n",
    "def r_k(y_actual, y_pred, k, threshold):\n",
    "    topK = min( len(y_pred), k ) # set top k\n",
    "    l_zip = list(zip(y_actual,y_pred))\n",
    "    # sort y_pred by the probability of the model\n",
    "    s_zip = sorted( l_zip, key=lambda x: x[1], reverse=True )\n",
    "    # topk of sorted result\n",
    "    s_zip_topk = s_zip[:topK] # Shape (5,2)\n",
    "    # print(s_zip_topk)\n",
    "    actual, pred = zip(*s_zip_topk)\n",
    "    actual = np.where(np.array(actual) > threshold, 1, 0)\n",
    "    pred = np.array(pred)\n",
    "    pred_o = np.where(pred > threshold, 1, 0)\n",
    "    return actual, pred_o, pred\n",
    "\n",
    "\n",
    "def user_evaluation_metrics(y_pred, y_test):\n",
    "    total_auc = 0\n",
    "    total_map = 0\n",
    "    total_recall = 0\n",
    "    v_count = 0\n",
    "    count = 0\n",
    "\n",
    "    y_pred = np.where(y_pred >0,1,0)\n",
    "    y_test = np.where(y_test >0,1,0)\n",
    "    for i in tqdm(range(y_test.shape[0])):\n",
    "        if (np.sum(y_test[i])> 0):\n",
    "            fpr, tpr, thresholds = skmetrics.roc_curve(y_test[i], y_pred[i], pos_label=1)\n",
    "            total_auc += skmetrics.auc(fpr, tpr)\n",
    "            actual, pred_o, pred = r_k(y_test[i], y_pred[i] ,5, 0.5)\n",
    "            total_recall += skmetrics.recall_score(actual, pred_o, average='macro') # Recall@5\n",
    "            total_map += cal_ap(y_test[i], y_pred[i], len(y_test[i]))\n",
    "            v_count +=1\n",
    "        else:\n",
    "            count +=1\n",
    "            pass\n",
    "        \n",
    "    total_auc = total_auc / v_count\n",
    "    total_map = total_map / v_count\n",
    "    total_recall = total_recall / v_count\n",
    "\n",
    "    return total_auc, total_map, total_recall, v_count, count\n",
    "        \n",
    "    # print('AUC: ', avg_auc / app_target.shape[0])\n",
    "    # print('MAP: ', avg_map / app_target.shape[0])\n",
    "    # print('Recall@5: ', avg_recall / app_target.shape[0])\n",
    "    # print('Skipped: {} rows, total: {} rows'.format(count, app_target.shape[0]))\n",
    "\n",
    "# fpr, tpr, thresholds = skmetrics.roc_curve(y_test, pred_MNB_prob, pos_label=1) # Collect the recall and false positive rate from all 2000 predictions\n",
    "# acc[0] += skmetrics.auc(fpr, tpr)\n",
    "# # acc[1] += cal_ap(truth, predict, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=40890.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19d07997c93a418c8565acf3cd450a75"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nAUC:  0.6640623867305869\nMAP:  0.34415445719949234\nRecall@5:  0.6419585011381499\nRows: 11422 / 40890\n"
     ]
    }
   ],
   "source": [
    "a,m,r,v,c= user_evaluation_metrics(app_np, app_target)\n",
    "\n",
    "print('AUC: ', a)\n",
    "print('MAP: ', m)\n",
    "print('Recall@5: ', r)\n",
    "print(\"Rows: {} / {}\".format(v,v+c))"
   ]
  }
 ]
}